{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Final-data/final_final.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la lista de names\n",
    "names = df['Name'].dropna().unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hacemos un web scraping de la api de wikipedia con el listado de los nombres de los asesinos y una serie de palabras clave\n",
    "crime_keywords = [\n",
    "    \"abduction\", \"arson\", \"beheading\", \"bestiality\", \"burglary\", \"cannibalism\", \"castration\", \"child abuse\", \"dismemberment\",\n",
    "    \"domestic violence\", \"hate crime\", \"incest\", \"kidnapping\", \"molestation\", \"murder\", \"necrophilia\", \"pedophilia\", \"rape\",\n",
    "    \"robbery\", \"sexual abuse\", \"sodomy\", \"serial killer\", \"stalking\", \"strangulation\", \"terrorism\", \"torture\"]\n",
    "\n",
    "def get_crimes_from_wikipedia_api(name):\n",
    "    try:\n",
    "        search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": name,\n",
    "            \"format\": \"json\"}\n",
    "        response = requests.get(search_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if not data['query']['search']:\n",
    "            return \"Not Found\"\n",
    "\n",
    "        title = data['query']['search'][0]['title']\n",
    "        page_url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "        page = requests.get(page_url)\n",
    "        if page.status_code != 200:\n",
    "            return \"Page Error\"\n",
    "\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        content = \" \".join([p.get_text() for p in paragraphs[:25]])\n",
    "\n",
    "        found_crimes = [kw for kw in crime_keywords if re.search(rf\"\\b{kw}\\b\", content.lower())]\n",
    "        return ', '.join(found_crimes) if found_crimes else \"murder\"\n",
    "    \n",
    "    except Exception:\n",
    "        return \"Error\"\n",
    "#Para evitar que se colapse wikipedia o que nos bloquee, hacemos las busquedas por lotes que uniremos cuando los tengamos todos\n",
    "batch_size = 100\n",
    "num_batches = math.ceil(len(names) / batch_size)\n",
    "batch_folder = \"Primary-data/batches_crimes\"\n",
    "os.makedirs(batch_folder, exist_ok=True)\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start = i * batch_size\n",
    "    end = start + batch_size\n",
    "    batch_names = names[start:end]\n",
    "    results = []\n",
    "\n",
    "    for name in batch_names:\n",
    "        crimes = get_crimes_from_wikipedia_api(name)\n",
    "        results.append({'Name': name, 'Crimes': crimes})\n",
    "        sleep(0.5)\n",
    "\n",
    "    batch_df = pd.DataFrame(results)\n",
    "    batch_df.to_csv(f\"{batch_folder}/crimes_batch_{i+1}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir todos los batches de crimines cometidos\n",
    "all_crimes_batches = []\n",
    "for file in sorted(os.listdir(\"Primary-data/batches_crimes\")):\n",
    "    if file.endswith(\".csv\"):\n",
    "        batch_path = os.path.join(\"Primary-data/batches_crimes\", file)\n",
    "        all_crimes_batches.append(pd.read_csv(batch_path))\n",
    "\n",
    "df_crimes = pd.concat(all_crimes_batches, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_keywords_map = {\n",
    "    \"death penalty\": \"Death penalty\", \"execution\": \"Execution\", \"life imprisonment\": \"Life imprisonment\",\n",
    "    \"life sentence\": \"Life imprisonment\", \"multiple life sentences\": \"Multiple life sentences\",\n",
    "    \"capital punishment\": \"Death penalty\", \"parole\": \"Parole\", \"released\": \"Released\", \"commuted\": \"Sentence commuted\",\n",
    "    \"incarcerated\": \"Incarcerated\", \"prison\": \"Incarcerated\", \"sentenced to\": \"Sentenced\",\n",
    "\n",
    "    \"unidentified\": \"Not identified\", \"not identified\": \"Not identified\", \"never caught\": \"Not identified\",\n",
    "    \"unknown killer\": \"Not identified\", \"unknown offender\": \"Not identified\",\n",
    "\n",
    "    \"committed suicide\": \"Died before sentence\", \"killed himself\": \"Died before sentence\",\n",
    "    \"died before trial\": \"Died before sentence\", \"died in custody\": \"Died before sentence\",\n",
    "    \"found dead\": \"Died before sentence\", \"shot himself\": \"Died before sentence\"\n",
    "}\n",
    "\n",
    "def get_sentence_from_wikipedia_api(name):\n",
    "    try:\n",
    "        search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": name,\n",
    "            \"format\": \"json\"}\n",
    "        response = requests.get(search_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if not data['query']['search']:\n",
    "            return \"Not Found\"\n",
    "\n",
    "        title = data['query']['search'][0]['title']\n",
    "        page_url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "        page = requests.get(page_url)\n",
    "        if page.status_code != 200:\n",
    "            return \"Page Error\"\n",
    "\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        content = \" \".join([p.get_text() for p in paragraphs[:25]]).lower()\n",
    "\n",
    "        found = set()\n",
    "        for keyword, label in sentence_keywords_map.items():\n",
    "            if re.search(rf\"\\b{re.escape(keyword)}\\b\", content):\n",
    "                found.add(label)\n",
    "\n",
    "        return ', '.join(found) if found else \"Unknown\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error con {name}: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "#Procesamos y guardamos por lotes para evitar que la api se enfade y nos bloquee\n",
    "batch_size = 100\n",
    "num_batches = math.ceil(len(names) / batch_size)\n",
    "batch_folder = \"Primary-data/batches_sentence\"\n",
    "os.makedirs(batch_folder, exist_ok=True)\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start = i * batch_size\n",
    "    end = start + batch_size\n",
    "    batch_names = names[start:end]\n",
    "    results = []\n",
    "\n",
    "    for name in batch_names:\n",
    "        penalty = get_sentence_from_wikipedia_api(name)\n",
    "        results.append({'Name': name, 'Sentence': penalty})  # usamos \"Sentence\" directamente\n",
    "\n",
    "        sleep(0.5)\n",
    "\n",
    "    batch_df = pd.DataFrame(results)\n",
    "    batch_df.to_csv(f\"{batch_folder}/sentence_batch_{i+1}.csv\", index=False)\n",
    "\n",
    "#unimos todos los batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir todos los batches de sentencias\n",
    "all_sentence_batches = []\n",
    "for file in sorted(os.listdir(\"Primary-data/batches_sentence\")):\n",
    "    if file.endswith(\".csv\"):\n",
    "        batch_path = os.path.join(\"Primary-data/batches_sentence\", file)\n",
    "        all_sentence_batches.append(pd.read_csv(batch_path))\n",
    "\n",
    "df_sentence = pd.concat(all_sentence_batches, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unimos los dos df que hemos sacado de las busqueda\n",
    "df_trial = pd.merge(df_crimes, df_sentence, on=\"Name\", how=\"outer\")\n",
    "df_trial.info()\n",
    "#y revisamos nulos tras el merge\n",
    "print(df_trial.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores únicos en 'Crimes':\")\n",
    "print(df_trial['Crimes'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nValores únicos en 'Sentence':\")\n",
    "print(df_trial['Sentence'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos una funcion para normalizar los calores de las columnas\n",
    "def clean_crimes_column(text):\n",
    "    if pd.isnull(text) or str(text).strip().lower() in [\"not found\", \"\", \"nan\"]:\n",
    "        return \"unknown\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z,\\s]', '', text)\n",
    "    text = re.sub(r'\\s*,\\s*', ',', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    crimes_list = list(dict.fromkeys(text.split(\",\")))\n",
    "    return \",\".join(crimes_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y la aplicamos\n",
    "df_trial[\"Crimes\"] = df_trial[\"Crimes\"].apply(clean_crimes_column)\n",
    "df_trial[\"Sentence\"] = df_trial[\"Sentence\"].apply(clean_crimes_column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificamos\n",
    "print(df_trial[\"Crimes\"].unique())\n",
    "print(df_trial[\"Sentence\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos las conciciones para considerar que una info no es relevante\n",
    "#En 'Crimes': si es \"Unknown\" o simplemente \"murder\", pues es demasiado genericodemasiado genérico\n",
    "no_info_crimes = df_trial['Crimes'].isin([\"Unknown\", \"murder\"])\n",
    "\n",
    "# En 'Sentence': si el scraping falló o no encontró nada útil\n",
    "no_info_penalties = df_trial['Sentence'].isin([\n",
    "    \"Unknown\", \"Not Found\", \"Page Error\", \"Error\"])\n",
    "\n",
    "#Teniendo eto, pasamos a filtrar los registros que cumplen ambas condiciones\n",
    "df_no_info = df_trial[no_info_crimes & no_info_penalties]\n",
    "\n",
    "display(df_no_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos el csv:\n",
    "output_path = \"Final-data/Trial_data\"\n",
    "df_trial.to_csv(output_path, index=False)\n",
    "print(f\"Archivo guardado correctamente en: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
