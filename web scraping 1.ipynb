{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "import math\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar archivo CSV\n",
    "df = pd.read_csv('Primary-data/killers-limpio.csv')\n",
    "names = df['Name'].dropna().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Ahora obtenemos la info del género desde el contenido de Wikipedia\n",
    "def infer_gender_from_wikipedia(name):\n",
    "    try:\n",
    "        clean_name = name.replace('\"', '').strip()\n",
    "\n",
    "        # Buscar el artículo más relevante usando la API\n",
    "        search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": clean_name,\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        response = requests.get(search_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if not data['query']['search']:\n",
    "            return \"Unknown\"\n",
    "\n",
    "        title = data['query']['search'][0]['title']\n",
    "        page_url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "        page = requests.get(page_url)\n",
    "        if page.status_code != 200:\n",
    "            return \"Unknown\"\n",
    "\n",
    "        # Extraer los primeros párrafos\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        content = \" \".join([p.get_text() for p in paragraphs[:3]]).lower()\n",
    "\n",
    "        # Reglas básicas basadas en pronombres\n",
    "        if any(w in content for w in [' she ', ' her ', 'herself']):\n",
    "            return 'female'\n",
    "        elif any(w in content for w in [' he ', ' his ', 'himself']):\n",
    "            return 'male'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "\n",
    "results = []\n",
    "for i, name in enumerate(names):\n",
    "    gender = infer_gender_from_wikipedia(name)\n",
    "    results.append({'Name': name, 'Gender': gender})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un nuevo df con resultados\n",
    "df_gender = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HAcemos una lista de palabras clave para detectar las armas urtilizadas por cada asesino.\n",
    "weapon_keywords = [\n",
    "    'gun', 'firearm', 'revolver', 'pistol', 'shot', 'shooting', 'knife', 'stabbed', 'stabbing', 'machete', 'axe',\n",
    "    'blunt object', 'hammer', 'bat', 'club', 'strangled', 'strangulation', 'garrote', 'choked', 'suffocated',\n",
    "    'poison', 'poisoned', 'arsenic', 'cyanide', 'chloroform', 'injected', 'hatchet', 'razor',\n",
    "    'fire', 'burned', 'burnt', 'set on fire', 'incinerated', 'drowned', 'drowning',\n",
    "    'explosives', 'bomb', 'grenade', 'decapitated', 'beheaded', 'hanged', 'hanging', 'sawed', 'cut into pieces',\n",
    "    'smothered', 'beat to death', 'run over', 'car', 'vehicle', 'truck','tortured', 'disemboweled', 'acid', 'chemical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapeo con API Wikipedia\n",
    "def get_weapon_from_wikipedia_api(name):\n",
    "    try:\n",
    "        clean_name = name.replace('\"', '').strip()\n",
    "        search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": clean_name,\n",
    "            \"format\": \"json\"}\n",
    "        response = requests.get(search_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if not data['query']['search']:\n",
    "            return \"Not Found\"\n",
    "\n",
    "        title = data['query']['search'][0]['title']\n",
    "        page_url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "        page = requests.get(page_url)\n",
    "        if page.status_code != 200:\n",
    "            return \"Page Error\"\n",
    "\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        content = \" \".join([p.get_text() for p in paragraphs[:25]])\n",
    "\n",
    "        found_weapons = [kw for kw in weapon_keywords if re.search(rf\"\\b{kw}\\b\", content.lower())]\n",
    "        return ', '.join(found_weapons) if found_weapons else \"Unknown\"\n",
    "    \n",
    "    except Exception:\n",
    "        return \"Error\"\n",
    "\n",
    "# Parámetros de lote\n",
    "batch_size = 100\n",
    "num_batches = math.ceil(len(names) / batch_size)\n",
    "\n",
    "# Crear carpeta para guardar resultados si no existe\n",
    "os.makedirs(\"Primary-data/batches_weapon\", exist_ok=True)\n",
    "\n",
    "# Ejecutar scraping por lotes\n",
    "for i in range(num_batches):\n",
    "    start = i * batch_size\n",
    "    end = start + batch_size\n",
    "    batch_names = names[start:end]\n",
    "    results = []\n",
    "\n",
    "    for name in batch_names:\n",
    "        weapon = get_weapon_from_wikipedia_api(name)\n",
    "        results.append({'Name': name, 'Weapon': weapon})\n",
    "        sleep(0.5)\n",
    "\n",
    "    batch_df = pd.DataFrame(results)\n",
    "    batch_df.to_csv(f\"Primary-data/batches_weapon/weapons_batch_{i+1}.csv\", index=False)\n",
    "\n",
    "# Unir todos los lotes en un solo CSV final\n",
    "all_batches = []\n",
    "for file in sorted(os.listdir(\"Primary-data/batches_weapon\")):\n",
    "    if file.endswith(\".csv\"):\n",
    "        batch_path = os.path.join(\"Primary-data/batches_weapon\", file)\n",
    "        all_batches.append(pd.read_csv(batch_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unimos todos los lotes del web scraping sobre armas\n",
    "df_weapons = pd.concat(all_batches, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos una funcion para normalizar los valores tras el scrapeo\n",
    "def clean_text_columns_custom(df):\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.replace('\"', '', regex=False)\n",
    "            .str.replace(\"'\", '', regex=False)\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        )\n",
    "\n",
    "        if col == \"Name\":\n",
    "            df[col] = df[col].str.title()\n",
    "        elif col in [\"Gender\", \"Weapon\"]:\n",
    "            df[col] = df[col].str.lower()\n",
    "    \n",
    "    return df\n",
    "\n",
    "#aplicamos la funcionnen los dos df\n",
    "df_gender = clean_text_columns_custom(df_gender)\n",
    "df_weapons = clean_text_columns_custom(df_weapons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unimos los resultados:\n",
    "df_scrapeo1 = pd.merge(df_gender, df_weapons, on=\"Name\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tras la union de los resultados vamos a ver si tenemos valores irregulares, duplicados o nulos.\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df_scrapeo1.isnull().sum())\n",
    "print(f\"duplicados completos{df_scrapeo1.duplicated().sum()}\")\n",
    "if duplicados.sum() > 0:\n",
    "    print(\"Ejemplos de duplicados:\")\n",
    "    print(df_scrapeo1[duplicados].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.loc[df_data['Name'] == 'Rainbow Maniac', 'Gender'] = 'male'\n",
    "#df_data.loc[df_data['Name'] == 'The Family', 'Gender'] = 'male'\n",
    "#df_data.loc[df_data['Name'] == 'Bian Kuang, Fu Xinyuan And Luo Lianshun', 'Gender'] = 'male'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#guardamos el csv:\n",
    "#output_path = \"scrapeo1\"\n",
    "#df_data.to_csv(output_path, index=False)\n",
    "#print(f\"Archivo guardado correctamente en: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
