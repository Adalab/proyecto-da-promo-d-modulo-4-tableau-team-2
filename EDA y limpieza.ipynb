{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el tratamiento de datos:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "import os\n",
    "pd.set_option('display.max_columns', None) #Para ver todas las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh = pd.read_csv(\"Primary-data/Highest_victim_count.csv\")\n",
    "df5 = pd.read_csv(\"Primary-data/15_to_30_victim_count.csv\")\n",
    "df4 = pd.read_csv(\"Primary-data/5_to_14_victim_count.csv\")\n",
    "dfl = pd.read_csv(\"Primary-data/Lessthan_5_victim_count.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comenzamos con un EDA inicial de los diferentes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh.head()  #Mas primeras 5 filas del DataFrame (por defecto, pero puedes especificar un n√∫mero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh.tail()  #Las √∫ltimas 5 filas del DataFrame (por defecto, pero puedes especificar un n√∫mero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh.sample()  #Una fila aleatoria del DataFrame (puedes especificar cu√°ntas filas con `n`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh.columns  #Los nombres de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh.shape #Una tupla con el n√∫mero de filas y columnas del DataFrame (filas, columnas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh.info()  #Info del DataFrame: n√∫mero de filas, columnas y tipos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh.dtypes #Tipo de datos de cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh.describe().T #Suma, media, desviaci√≥n estandar, m√≠nimo/m√°ximo y percentiles (valores num√©ricos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh.describe(include = 'object') #Suma, valores √∫nicos... de tipo object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bucle con el que extraemos datos relevantes de cada columna:\n",
    "columnas = dfh.columns\n",
    "\n",
    "for col in columnas:\n",
    "    print(f\"\\nüîπ'{col}'\\n\")\n",
    "    print(f\"Tipo de dato: {dfh[col].dtypes}\\n\")\n",
    "    print(f\"Porcentaje de nulos: {dfh[col].isnull().sum()/dfh.shape[0]*100}\\n\")\n",
    "    print(f\"Suma de duplicados: {dfh[col].duplicated().sum()}\\n\")\n",
    "    print(f\"Valores √∫nicos:\")\n",
    "    print(dfh[col].unique())\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head()  #Mas primeras 5 filas del DataFrame (por defecto, pero puedes especificar un n√∫mero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.tail()  #Las √∫ltimas 5 filas del DataFrame (por defecto, pero puedes especificar un n√∫mero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.sample()  #Una fila aleatoria del DataFrame (puedes especificar cu√°ntas filas con `n`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.columns  #Los nombres de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.shape #Una tupla con el n√∫mero de filas y columnas del DataFrame (filas, columnas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.info()  #Info del DataFrame: n√∫mero de filas, columnas y tipos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.dtypes #Tipo de datos de cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.describe().T #Suma, media, desviaci√≥n estandar, m√≠nimo/m√°ximo y percentiles (valores num√©ricos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.describe(include = 'object') #Suma, valores √∫nicos... de tipo object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bucle con el que extraemos datos relevantes de cada columna:\n",
    "columnas = df5.columns\n",
    "\n",
    "\n",
    "for col in columnas:\n",
    "    print(f\"\\nüîπ'{col}'\\n\")\n",
    "    print(f\"Tipo de dato: {df5[col].dtypes}\\n\")\n",
    "    print(f\"Porcentaje de nulos: {df5[col].isnull().sum()/df5.shape[0]*100}\\n\")\n",
    "    print(f\"Suma de duplicados: {df5[col].duplicated().sum()}\\n\")\n",
    "    print(f\"Valores √∫nicos:\")\n",
    "    print(df5[col].unique())\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head()  #Mas primeras 5 filas del DataFrame (por defecto, pero puedes especificar un n√∫mero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.tail()  #Las √∫ltimas 5 filas del DataFrame (por defecto, pero puedes especificar un n√∫mero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.sample()  #Una fila aleatoria del DataFrame (puedes especificar cu√°ntas filas con `n`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.columns  #Los nombres de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.shape #Una tupla con el n√∫mero de filas y columnas del DataFrame (filas, columnas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.info()  #Info del DataFrame: n√∫mero de filas, columnas y tipos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.dtypes #Tipo de datos de cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.describe() #Suma, media, desviaci√≥n estandar, m√≠nimo/m√°ximo y percentiles (valores num√©ricos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.describe(include = 'object') #Suma, valores √∫nicos... de tipo object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bucle con el que extraemos datos relevantes de cada columna:\n",
    "columnas = df4.columns\n",
    "\n",
    "for col in columnas:\n",
    "    print(f\"\\nüîπ'{col}'\\n\")\n",
    "    print(f\"Tipo de dato: {df4[col].dtypes}\\n\")\n",
    "    print(f\"Porcentaje de nulos: {df4[col].isnull().sum()/df4.shape[0]*100}\\n\")\n",
    "    print(f\"Suma de duplicados: {df4[col].duplicated().sum()}\\n\")\n",
    "    print(f\"Valores √∫nicos:\")\n",
    "    print(df4[col].unique())\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.head()  #Mas primeras 5 filas del DataFrame (por defecto, pero puedes especificar un n√∫mero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.tail()  #Las √∫ltimas 5 filas del DataFrame (por defecto, pero puedes especificar un n√∫mero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.sample()  #Una fila aleatoria del DataFrame (puedes especificar cu√°ntas filas con `n`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.columns  #Los nombres de las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.shape #Una tupla con el n√∫mero de filas y columnas del DataFrame (filas, columnas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.info()  #Info del DataFrame: n√∫mero de filas, columnas y tipos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.dtypes #Tipo de datos de cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.describe() #Suma, media, desviaci√≥n estandar, m√≠nimo/m√°ximo y percentiles (valores num√©ricos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl.describe(include = 'object') #Suma, valores √∫nicos... de tipo object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bucle con el que extraemos datos relevantes de cada columna:\n",
    "columnas = dfl.columns\n",
    "\n",
    "for col in columnas:\n",
    "    print(f\"\\nüîπ'{col}'\\n\")\n",
    "    print(f\"Tipo de dato: {dfl[col].dtypes}\\n\")\n",
    "    print(f\"Porcentaje de nulos: {dfl[col].isnull().sum()/dfl.shape[0]*100}\\n\")\n",
    "    print(f\"Suma de duplicados: {dfl[col].duplicated().sum()}\\n\")\n",
    "    print(f\"Valores √∫nicos:\")\n",
    "    print(dfl[col].unique())\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A partir de aqui hacemos varias comprovaciones para asegurarnos que los datasets son iguales y se puede concatenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfh.shape)\n",
    "dfh['Name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df5.shape)\n",
    "df5['Name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df4.shape)\n",
    "df4['Name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfl.shape)\n",
    "dfl['Name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasamos a la limpieza y normalizacion de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar los df:\n",
    "dfc = pd.concat([dfh, df5, df4, dfl], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar duplicados\n",
    "dfc.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar los espacios por \"_\":\n",
    "dfc.columns #Me da todas las columnas\n",
    "\n",
    "# Diccionario vac√≠o:\n",
    "col_f = {} \n",
    "\n",
    "# Itera dfc.columns y crea un diccionario clave : valor con el cambio.\n",
    "for columna in dfc.columns:\n",
    "    col_f[columna] = columna.replace(\" \", \"_\")\n",
    "\n",
    "# Rename:\n",
    "dfc.rename(columns = col_f, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['Years_active'].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir 'Years_active' en 'Start Stop' y modificar la columna 'Years_active' (total a√±os en activo)\n",
    "# Crear la columna 'Start_year' a partir de 'Years_active': Eliminando el 'to' y lo que va despu√©s.\n",
    "dfc['Start_year'] = dfc['Years_active'].str.split(\" to \").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['Start_year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las columnas son una caca y tuve que preguntarle a chatgpt qu√© hacer, despu√©s de provar mil cosas me di√≥ este c√≥digo:\n",
    "# Extraer los cuatro primero d√≠gitos de la cadena:\n",
    "dfc['Start_year'] = dfc['Start_year'].str.extract(r'(\\d{4})')\n",
    "\n",
    "# Convertir la columna a num√©rico:\n",
    "dfc['Start_year'] = pd.to_numeric(dfc['Start_year'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar que todo ok:\n",
    "dfc['Start_year'].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para 'End_year' tambi√©n era un l√≠o as√≠ que me ayud√≥ de nuevo chatgpt.\n",
    "# Extraer todos los n√∫meros de 4 d√≠gitos en una lista para cada registro y tomar solamente el √∫ltimo, si no hay devuelve nulo.\n",
    "dfc['End_year'] = dfc['Years_active'].str.findall(r'(\\d{4})').apply(lambda x: x[-1] if x else np.nan)\n",
    "\n",
    "# Convertir la columna a num√©rico\n",
    "dfc['End_year'] = pd.to_numeric(dfc['End_year'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar again que todo ok:\n",
    "dfc['End_year'].to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar las dos columnas:\n",
    "dfc['Start_year'].to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al comparar los datos de 'Years_active', 'Start_year' y 'End_year' hay que cambiar ciertas entradas en 'End_year':\n",
    "- A 1959 en_ 92, 108 y 27. Ya que estuvieron activos en 1950s.\n",
    "- A 1999 en: 58 y 242. Ya que estuvieron activos en 1990s.\n",
    "- A 2009 en: 224 y 225. Ya que estuvieron activos en 2000s.\n",
    "- En el id 198 indica \"to present\", ¬øasumimos que la fecha es 2018 ya que es el √∫ltimo a√±o de actividad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver cu√°l es el √∫ltimo a√±o:\n",
    "unique_values = np.sort(dfc['End_year'].unique())[::-1]\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambios en 'End_year'.\n",
    "# A 1959 en_ 92, 108 y 27. Ya que estuvieron activos en 1950s.\n",
    "dfc.loc[[92, 108, 27], 'End_year'] = 1959\n",
    "\n",
    "# A 1999 en: 58 y 242. Ya que estuvieron activos en 1990s.\n",
    "dfc.loc[[58, 242], 'End_year'] = 1999\n",
    "\n",
    "# A 2009 en: 24 y 25. Ya que estuvieron activos en 2000s.\n",
    "dfc.loc[[224, 225], 'End_year'] = 2009\n",
    "\n",
    "# En el id 198 indica \"to present\", ¬øasumimos que la fecha es 2018 ya que es el √∫ltimo a√±o de actividad?\n",
    "dfc.loc[[198], 'End_year'] = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columna nueva con a√±os en activo, nuevo nombre para que no se solape:\n",
    "# A√±adimos el +1 para que los que llevan solo un a√±o en activo no se quede 0 y para que el primer a√±o tambi√©n cuente en el resto.\n",
    "dfc['A√±os_activo'] = dfc['End_year'] - dfc['Start_year'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['A√±os_activo'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar 'Years_active' para despu√©s cambiar 'A√±os_activo':\n",
    "dfc.drop('Years_active', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambio de nombre:\n",
    "dfc.rename(columns = {'A√±os_activo': 'Years_active'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora con 'Proven_victims'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['Proven_victims'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hago ua funci√≥n para cambiar los valores que \"no sirven\" porque intentando hacerlo con c√≥digo la liaba siempre.\n",
    "def intervalos(Proven_victims):\n",
    "    if Proven_victims == '11‚Äì16':\n",
    "        return \"11\"\n",
    "    elif Proven_victims == \"9‚Äì13\":\n",
    "        return \"9\"\n",
    "    elif Proven_victims == \"8‚Äì15\":\n",
    "        return \"8\"\n",
    "    elif Proven_victims == \"8‚Äì11+\":\n",
    "        return \"8\"\n",
    "    else:\n",
    "        return Proven_victims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica la funci√≥n:\n",
    "dfc['Proven_victims'] = dfc['Proven_victims'].apply(intervalos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviso:\n",
    "dfc['Proven_victims'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambio a int:\n",
    "dfc['Proven_victims'] = dfc['Proven_victims'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviso again:\n",
    "dfc['Proven_victims'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos encontramos que en los registros de la columna country aparecen listas de paises separados por saltos de fila. Tableau no puede interpretar estos datos correctamente \n",
    "por lo que tr√°s hablar con la PO tomamos la decision de hacer un .explode por filas y quedarnos con la primera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfc['Country'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos los pa√≠ses por salto de l√≠nea, explotamos filas y limpiamos espacios delante y detr√°s\n",
    "dfc['Country'] = dfc['Country'].str.split(r'\\r\\n')\n",
    "dfc = dfc.explode('Country')\n",
    "dfc['Country'] = dfc['Country'].str.strip()\n",
    "dfc = dfc.drop_duplicates(subset='Name', keep='first').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = dfc.duplicated(subset='Name').sum()\n",
    "print(f'Duplicados encontrados: {duplicates}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comprobamos que se ha ejecutado correctamente\n",
    "print(sorted(dfc['Country'].unique()))\n",
    "print(dfc['Country'].isnull().sum())\n",
    "dfc['Country'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que tras la limpieza nuestro dataset son pocos registros pasa analizar y a√±adimos dos csv m√°s para completar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_victims = pd.read_csv(\"Primary-data/serial_killers_by_victims.csv\", sep=\";\")\n",
    "df_profile = pd.read_csv(\"Primary-data/profile_killers.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile.info()\n",
    "df_victims.info()\n",
    "dfc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columnas en df_victims:\")\n",
    "print(df_victims.columns)\n",
    "\n",
    "print(\"Columnas en df_profile:\")\n",
    "print(df_profile.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_killers = df_victims.merge(df_profile, on='Name', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar los espacios por \"_\":\n",
    "df_new_killers.columns\n",
    "\n",
    "col_f = {} \n",
    "\n",
    "# Itera dfc.columns y crea un diccionario clave : valor con el cambio.\n",
    "for columna in df_new_killers.columns:\n",
    "    col_f[columna] = columna.replace(\" \", \"_\")\n",
    "\n",
    "# Rename:\n",
    "df_new_killers.rename(columns = col_f, inplace = True)\n",
    "\n",
    "#vemos los cambios en columnas\n",
    "print(df_new_killers.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_killers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VEr uplicados por nombre\n",
    "duplicados = df_new_killers[df_new_killers.duplicated(subset='Name', keep=False)]\n",
    "print(f\"Duplicados por 'Name': {duplicados.shape[0]}\")\n",
    "if not duplicados.empty:\n",
    "    print(duplicados[['Name', 'Country', 'Start_year', 'End_year']])\n",
    "\n",
    "#Ver nulos por columna\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df_new_killers.isnull().sum())\n",
    "\n",
    "#Ver si End_year es menor que Start_year\n",
    "errores_fecha = df_new_killers[df_new_killers['End_year'] < df_new_killers['Start_year']]\n",
    "print(f\"End_year < Start_year: {errores_fecha.shape[0]}\")\n",
    "if not errores_fecha.empty:\n",
    "    print(errores_fecha[['Name', 'Start_year', 'End_year']])\n",
    "\n",
    "#Ver si el n√∫mero de victimas es negativo o 0\n",
    "errores_victimas = df_new_killers[df_new_killers['Proven_victims'] <= 0]\n",
    "print(f\"Proven_victims <= 0: {errores_victimas.shape[0]}\")\n",
    "if not errores_victimas.empty:\n",
    "    print(errores_victimas[['Name', 'Proven_victims']])\n",
    "\n",
    "#Ver si hay registros sin nombre\n",
    "sin_nombre = df_new_killers['Name'].isnull().sum()\n",
    "print(f\"Registros sin nombre: {sin_nombre}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VER columnas disponibles en ambos\n",
    "print(dfc.columns)\n",
    "print(df_new_killers.columns)\n",
    "\n",
    "#Hacemos merge con outer n para mantener todos los asesinos\n",
    "df_final = pd.merge(dfc, df_new_killers, on='Name', how='outer', suffixes=('_kaggle', '_extra'))\n",
    "\n",
    "#Ver si hay coincidencias filtrando por la columna name\n",
    "matches = dfc['Name'].isin(df_new_killers['Name']).sum()\n",
    "print(f\"Coincidencias por 'Name': {matches} de {len(dfc)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ya sabemos que Name es identificador √∫nico, ahora vamos a ver si hay duplicados por nombre.\n",
    "duplicados = df_final[df_final.duplicated(subset='Name', keep=False)]\n",
    "\n",
    "#En el caso de que haya, quiero ver los nombres que estan duplicados y cuantas veces se repite\n",
    "nombres_duplicados = duplicados['Name'].value_counts()\n",
    "print(f\"Total asesinos con duplicados: {nombres_duplicados.shape[0]}\")\n",
    "print(\"Nombres duplicados (con cantidad de repeticiones):\")\n",
    "print(nombres_duplicados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos las columnas comunes que tienen sufijos _kaggle y _extra\n",
    "columnas_kaggle = [col for col in df_final.columns if col.endswith('_kaggle')]\n",
    "columnas_extra = [col for col in df_final.columns if col.endswith('_extra')]\n",
    "\n",
    "columnas_comunes = []\n",
    "for col_k in columnas_kaggle:\n",
    "    base_name = col_k.replace('_kaggle', '')\n",
    "    col_e = base_name + '_extra'\n",
    "    if col_e in columnas_extra:\n",
    "        columnas_comunes.append(base_name)\n",
    "\n",
    "#Analizar ambas direcciones\n",
    "print(\"Aportes de datos entre _kaggle y _extra:\")\n",
    "for col in columnas_comunes:\n",
    "    col_kaggle = f\"{col}_kaggle\"\n",
    "    col_extra = f\"{col}_extra\"\n",
    "\n",
    "    aporta_kaggle = df_final[col_extra].isnull() & df_final[col_kaggle].notnull()\n",
    "    aporta_extra = df_final[col_kaggle].isnull() & df_final[col_extra].notnull()\n",
    "\n",
    "    print(f\" {col}\")\n",
    "    print(f\"_kaggle aporta en: {aporta_kaggle.sum()} registros donde _extra est√° vac√≠o\")\n",
    "    print(f\"_extra aporta en: {aporta_extra.sum()} registros donde _kaggle est√° vac√≠o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consolidamos las columnas rellenando los datos faltantes de una columna y otra y eliminamos\n",
    "columnas_a_consolidar = ['Country', 'Proven_victims', 'Possible_victims', 'Notes', 'Start_year', 'End_year']\n",
    "\n",
    "for col in columnas_a_consolidar:\n",
    "    df_final[col] = df_final[f\"{col}_extra\"].combine_first(df_final[f\"{col}_kaggle\"])\n",
    "    df_final.drop(columns=[f\"{col}_kaggle\", f\"{col}_extra\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculamos los registros faltates de Years_active \n",
    "df_final['Years_active'] = df_final['End_year'] - df_final['Start_year']\n",
    "#y pasamos posibles victimas a numerico\n",
    "df_final['Possible_victims'] = pd.to_numeric(df_final['Possible_victims'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como ya sabemos que Name es un id unico vamos a filtrar los asesinos de los que solo tenemos esa informacion para poder eliminarlos.\n",
    "solo_nombre = df_final.drop(columns='Name').isnull().all(axis=1)\n",
    "df_final = df_final[~solo_nombre].reset_index(drop=True)\n",
    "print(f\"Se eliminaron {solo_nombre.sum()} registros con solo el nombre.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La columna country es importante, vamos a pasar a limpiarla y a procesar los nulos.\n",
    "#podemos usar Born_location para imputar los nulos de country\n",
    "filtro = df_final['Country'].isnull() & df_final['Born_Location'].notnull()\n",
    "#Sacamos el pa√≠s que es la ultima parte de string\n",
    "df_final.loc[filtro, 'Country'] = df_final.loc[filtro, 'Born_Location'].apply(lambda x: x.split(',')[-1].strip())\n",
    "# Verificar cu√°ntos se completaron\n",
    "completados = filtro.sum()\n",
    "print(f\" Pa√≠s estimado desde Born_Location en {completados} registros.\")\n",
    "df_final['Country'].value_counts()\n",
    "#DE nuevo encontramos paises que esta en una lista, repetimos el codigo que ejecutamos anteriormente para hacer registro unico por asesino y pais y luegos nos quedamos con la primera aparicion, las medas las aliminamos.\n",
    "#Separamos los pa√≠ses multiples por coma y explotamos las filas\n",
    "df_final['Country'] = df_final['Country'].str.split(',')\n",
    "df_final = df_final.explode('Country')\n",
    "#eliminamos espacios en blanco\n",
    "df_final['Country'] = df_final['Country'].str.strip()\n",
    "#dejamos solo una fila por asesino\n",
    "df_final = df_final.drop_duplicates(subset='Name', keep='first').reset_index(drop=True)\n",
    "#Pasamos a normalizar los paises, que hay mucho valores inconsistentes. Primero creamos un diccionario que le he pedido a chat gepete que me haga porque esto es perder la vida\n",
    "normalizacion_paises = {\n",
    "    # Variantes y abreviaturas\n",
    "    \"USA\": \"United States\",\n",
    "    \"US\": \"United States\",\n",
    "    \"United States of America\": \"United States\",\n",
    "    \"United States of Brazil\": \"Brazil\",\n",
    "    \"Rio de Janeiro\": \"Brazil\",\n",
    "    \"So Paulo\": \"Brazil\",\n",
    "    \"Mxico\": \"Mexico\",\n",
    "    \"Uk\": \"United Kingdom\",\n",
    "    \"UK\": \"United Kingdom\",\n",
    "    \"England\": \"United Kingdom\",\n",
    "    \"Scotland\": \"United Kingdom\",\n",
    "    \"Ontario Canada\": \"Canada\",\n",
    "    \"Territory of Hawaii\": \"United States\",\n",
    "    \"British Hong Kong\": \"Hong Kong\",\n",
    "    \"Imperial State of Iran\": \"Iran\",\n",
    "\n",
    "    # Hist√≥ricos o alternativos\n",
    "    \"Allied-occupied Germany\": \"Germany\",\n",
    "    \"West Germany\": \"Germany\",\n",
    "    \"East Germany\": \"Germany\",\n",
    "    \"Nazi Germany\": \"Germany\",\n",
    "    \"German Empire, Germany\": \"Germany\",\n",
    "    \"Soviet Union\": \"Russia\",\n",
    "    \"Soviet Union, Russia\": \"Russia\",\n",
    "    \"Soviet Union, Ukraine\": \"Ukraine\",\n",
    "    \"Soviet Union, Russia, Ukraine\": \"Russia\",\n",
    "    \"Uzbek SSR\": \"Uzbekistan\",\n",
    "    \"UkSSR\": \"Ukraine\",\n",
    "    \"Ottoman Empire\": \"Turkey\",\n",
    "    \"centuryOttoman Empire\": \"Turkey\",\n",
    "    \"Austria-Hungary\": \"Austria\",\n",
    "    \"Austro-Hungarian Empire\": \"Austria\",\n",
    "    \"Czechoslovakia\": \"Czech Republic\",\n",
    "    \"SR Croatia\": \"Croatia\",\n",
    "    \"SR Slovenia\": \"Slovenia\",\n",
    "    \"Yugoslavia\": \"Serbia\",\n",
    "    \"Kingdom of Yugoslavia\": \"Serbia\",\n",
    "    'Kingdom of Romania': 'Romania',\n",
    "    \"Kingdom of Romania, Yugoslavia, Hungary\": \"Romania\",\n",
    "    \"Cape Colony\": \"South Africa\",\n",
    "\n",
    "    # Errores o indefinidos\n",
    "    \"TaeiIraq\": \"Iraq\"\n",
    "}\n",
    "# Ejecutamos el diccionario \n",
    "df_final['Country'] = df_final['Country'].replace(normalizacion_paises)\n",
    "\n",
    "print(\"Pa√≠ses √∫nicos despu√©s de normalizar:\")\n",
    "print(sorted(df_final['Country'].dropna().astype(str).unique()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continuamos filtrando los asesinos que tiene valores chungos en la columna pa√≠s\n",
    "# Filtrar asesinos con Country en los valores problem√°ticos\n",
    "valores_chungos = ['s Republic', 'unknown']\n",
    "df_chungos = df_final[df_final['Country'].isin(valores_chungos)]\n",
    "display(df_chungos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputamos los datos faltantes de estos dos asesinos con datos de wikipedia:\n",
    "df_final.loc[df_final['Name'] == 'Monster Of Udine', [\n",
    "    'Nicknames', 'Conviction', 'Criminal_Penalty', 'Country',\n",
    "    'Proven_victims', 'Possible_victims', 'Start_year', 'End_year', 'Notes']] = [\n",
    "    'Monster Of Udine',             # Nicknames\n",
    "    'No conviction',                # Conviction\n",
    "    'Untried',                      # Criminal_Penalty\n",
    "    'Italy',                        # Country\n",
    "    14,                             # Proven_victims\n",
    "    10,                             # Possible_victims\n",
    "    1971,                           # Start_year\n",
    "    1989,                           # End_year\n",
    "    'The Monster of Udine is the nickname given to an unidentified killer. Most of the victims were prostitutes. The criminal was never identified, and the weapon was never found']\n",
    "\n",
    "df_final.loc[df_final['Name'] == 'Tadeusz Grzesik', [\n",
    "    'Nicknames', 'Conviction', 'Criminal_Penalty', 'Country',\n",
    "    'Proven_victims', 'Possible_victims', 'Start_year', 'End_year',\n",
    "    'Born_Date', 'Born_Location', 'Notes'\n",
    "]] = [\n",
    "    'Strawberry',                        # Nicknames\n",
    "    'Murder',                            # Conviction\n",
    "    'Life imprisonment',                 # Criminal_Penalty\n",
    "    'Poland',                            # Country\n",
    "    8,                                   # Proven_victims\n",
    "    20,                                  # Possible_victims\n",
    "    1991,                                # Start_year\n",
    "    2007,                                # End_year\n",
    "    '1960-01-01',                        # Born_Date\n",
    "    'Wola Jachowa, Poland',              # Born_Location\n",
    "    'A Polish strawberry grower and serial killer, known as \"Strawberry\" and \"Killer\". He led the Moneychangers Gang']\n",
    "\n",
    "#verificamos\n",
    "df_final[df_final['Name'] == 'Monster Of Udine']\n",
    "df_final[df_final['Name'] == 'Tadeusz Grzesik']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar registros con Country nulo\n",
    "nulos_country = df_final[df_final['Country'].isnull()]\n",
    "display(nulos_country)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesti√≥n de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ver nulos por columna\n",
    "total = len(df_final)\n",
    "nulos_df = pd.DataFrame({\n",
    "    'Nulos': df_final.isnull().sum(),\n",
    "    'Porcentaje': (df_final.isnull().sum() / total * 100).round(2)}).sort_values(by='Nulos', ascending=False)\n",
    "print(\"Nulos y porcentaje por columna:\")\n",
    "print(nulos_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Posible_victims y Proven_victims son esenciales para el analisis por lo que pasamos a borrar los registros que no tienen esos datos.\n",
    "registros_sin_victimas = df_final['Proven_victims'].isnull() & df_final['Possible_victims'].isnull()\n",
    "df_final = df_final[~registros_sin_victimas].reset_index(drop=True)\n",
    "#comprobamos viendo de nuevo los nulos.\n",
    "filtro = df_final['Proven_victims'].isnull() | df_final['Possible_victims'].isnull()\n",
    "df_final.loc[filtro, ['Name', 'Proven_victims', 'Possible_victims']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le paso nos nombre a chat gepete y le pido que busque informacion sobres las posibles victimas. \n",
    "#Tras la b√∫squeda decidimos que los asesinos que  no tenemos datos concisos se sustituya el nulo por 0 y en los casos que si tengamos datos se tomen esos datos para imputar los nulos.\n",
    "df_final.loc[df_final['Name'] == 'Jos√© Paz Bezerra', ['Possible_victims']] = [24]\n",
    "df_final.loc[df_final['Name'] == 'Rainbow Maniac', ['Possible_victims']] = [16]\n",
    "df_final['Possible_victims'] = df_final['Possible_victims'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volvemos a revisar nulos por columna\n",
    "total = len(df_final)\n",
    "nulos_df = pd.DataFrame({\n",
    "    'Nulos': df_final.isnull().sum(),\n",
    "    'Porcentaje': (df_final.isnull().sum() / total * 100).round(2)}).sort_values(by='Nulos', ascending=False)\n",
    "print(\"Nulos y porcentaje por columna:\")\n",
    "print(nulos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columnas como la ubicacion de nacimiento y muerte, tiene un alto porcentaje de nulos y no las vemos √∫tiles para el analisis. Decidimos borrarlas\n",
    "df_final.drop(columns=['Died_Location', 'Born_Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar registros con Years_active vac√≠o\n",
    "nulos_year = df_final[df_final['Years_active'].isnull()]\n",
    "print(nulos_year.shape[0])\n",
    "display(nulos_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 registros de End_year y start_year son asumibles. Pido a chat gepete que busque la info para esos asesinos, e imputar asi esos datos.\n",
    "df_final.loc[df_final['Name'] == 'Baba Anujka', 'End_year'] = 1928\n",
    "df_final.loc[df_final['Name'] == 'John Duffy and David Mulcahy', 'End_year'] = 1986\n",
    "df_final.loc[df_final['Name'] == 'Murder Incorporated', 'End_year'] = 1940\n",
    "df_final.loc[df_final['Name'] == 'Abboud and Khajawa', 'Start_year'] = 1917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combinando las columnas years_active, start_year y end_year podemos intentar completar los datos nulos de las mismas\n",
    "# Rellenar Years_active si est√° vac√≠o y hay Start y End\n",
    "df_final['Years_active'] = df_final['Years_active'].fillna(df_final['End_year'] - df_final['Start_year'])\n",
    "\n",
    "#Podemos seguir la misma estrategia con las columnas Age, Born_Date y Died_Date \n",
    "def calcular_edad(born, died):\n",
    "    try:\n",
    "        born = pd.to_datetime(born, errors='coerce')\n",
    "        died = pd.to_datetime(died, errors='coerce')\n",
    "        if pd.notnull(born) and pd.notnull(died):\n",
    "            return died.year - born.year\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_final['Age_calculada'] = df_final.apply(lambda row: calcular_edad(row['Born_Date'], row['Died_Date']),axis=1)\n",
    "df_final['Age'] = df_final['Age'].fillna(df_final['Age_calculada'])\n",
    "df_final.drop(columns='Age_calculada', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Volvemos a revisar nulos por columna\n",
    "total = len(df_final)\n",
    "nulos_df = pd.DataFrame({\n",
    "    'Nulos': df_final.isnull().sum(),\n",
    "    'Porcentaje': (df_final.isnull().sum() / total * 100).round(2)}).sort_values(by='Nulos', ascending=False)\n",
    "print(\"Nulos y porcentaje por columna:\")\n",
    "print(nulos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas que tienen muchos nulos\n",
    "df_final.drop(columns=['Age', 'Died_Date', 'Born_Date', 'Date_Apprehended'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reordenar columnas\n",
    "column_order = [\"Name\", \"Nicknames\", \"Country\", \"Proven_victims\", \"Possible_victims\", \"Years_active\", \"Start_year\", \"End_year\", \"Conviction\", \"Criminal_Penalty\", \"Notes\"]\n",
    "df_final = df_final[column_order]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que nuestro data set es un tanto escaso y decidimos completar con informaci√≥n haciendo un web scraping del sexo de los asesinos as√≠ como de las armas utilizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrapeo1 = pd.read_csv('Primary-data/scrapeo1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrapeo1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_final[\"Name\"] = df_final[\"Name\"].str.replace('\"', '').str.strip().str.title()\n",
    "df_scrapeo1[\"Name\"] = df_scrapeo1[\"Name\"].str.replace('\"', '').str.strip().str.title()\n",
    "df_completo = pd.merge(df_final, df_scrapeo1, on=\"Name\", how=\"left\")\n",
    "print(df_completo.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra los registros donde Weapon o Gender sean nulos\n",
    "nulos_weapon_gender = df_completo[df_completo[\"Weapon\"].isna() | df_completo[\"Gender\"].isna()]\n",
    "\n",
    "# Muestra resultados\n",
    "print(f\"Registros con NaN en Weapon o Gender: {nulos_weapon_gender.shape[0]}\")\n",
    "display(nulos_weapon_gender)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de nombres a eliminar por falta de datos esenciales.\n",
    "nombres_a_eliminar = [\n",
    "    \"John Justin Bunting, Robert Joe Wagner And James Vlassakis\",\n",
    "    \"Satish\",\n",
    "    \"Silvia Merez Cult\"]\n",
    "\n",
    "# Eliminamos del dataframe\n",
    "df_completo = df_completo[~df_completo[\"Name\"].isin(nombres_a_eliminar)]\n",
    "\n",
    "# Confirmamos que ya no est√°n\n",
    "print(df_completo[df_completo[\"Name\"].isin(nombres_a_eliminar)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos a quitar los acentos que nos dan problemas.\n",
    "def quitar_acentos(texto):\n",
    "    if pd.isna(texto):\n",
    "        return texto\n",
    "    return unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "df_completo['Name'] = df_completo['Name'].apply(quitar_acentos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vemos los valores unicos de weapon para poder establecer categorias para que sea m√°s facil el analisis en tableau\n",
    "valores_weapon = sorted(df_completo[\"Weapon\"].dropna().unique())\n",
    "for weapon in valores_weapon:\n",
    "    print(weapon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizar_weapon(valor):\n",
    "    if pd.isna(valor):\n",
    "        return \"Unknown\"\n",
    "    valor = valor.lower()\n",
    "    if any(p in valor for p in [\"gun\", \"pistol\", \"revolver\", \"shotgun\", \"shooting\", \"shot\"]):\n",
    "        return \"Firearm\"\n",
    "    elif any(p in valor for p in [\"knife\", \"machete\", \"razor\", \"stabbed\", \"stabbing\"]):\n",
    "        return \"Blade\"\n",
    "    elif any(p in valor for p in [\"club\", \"bat\", \"hammer\", \"blunt object\"]):\n",
    "        return \"Blunt Object\"\n",
    "    elif any(p in valor for p in [\"strangled\", \"strangulation\", \"garrote\", \"choked\", \"smothered\", \"suffocated\"]):\n",
    "        return \"Strangulation\"\n",
    "    elif any(p in valor for p in [\"poison\", \"poisoned\", \"arsenic\", \"cyanide\", \"chemical\", \"chloroform\"]):\n",
    "        return \"Poison\"\n",
    "    elif any(p in valor for p in [\"car\", \"truck\", \"vehicle\"]):\n",
    "        return \"Vehicle\"\n",
    "    elif any(p in valor for p in [\"fire\", \"burned\", \"set on fire\", \"burnt\", \"explosives\", \"bomb\", \"grenade\"]):\n",
    "        return \"Fire\"\n",
    "    elif any(p in valor for p in [\"drowned\", \"drowning\"]):\n",
    "        return \"Drowning\"\n",
    "    elif any(p in valor for p in [\"beheaded\", \"decapitated\"]):\n",
    "        return \"Beheading\"\n",
    "    elif any(p in valor for p in [\"injected\", \"drugs\", \"medication\"]):\n",
    "        return \"Drugs\"\n",
    "    elif any(p in valor for p in [\"tortured\", \"beat to death\", \"incinerated\", \"disemboweled\"]):\n",
    "        return \"Torture\"\n",
    "    elif any(p in valor for p in [\"hanged\", \"hanging\"]):\n",
    "        return \"Hanging\"\n",
    "    elif valor == \"unknown\":\n",
    "        return \"Unknown\"\n",
    "    else:\n",
    "        return \"Unknown\" \n",
    "\n",
    "\n",
    "df_completo[\"Weapon_Category\"] = df_completo[\"Weapon\"].apply(categorizar_weapon)\n",
    "print(df_completo[\"Weapon_Category\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el CSV final \n",
    "os.makedirs(\"Final-data\", exist_ok=True)\n",
    "output_path = \"Final-data/final_final.csv\"\n",
    "df_completo.to_csv(output_path, index=False)\n",
    "print(f\"Archivo guardado correctamente en: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
